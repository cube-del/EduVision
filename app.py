import gradio as gr
import torch
import torchvision.transforms as T
from PIL import Image
from torchvision.transforms.functional import InterpolationMode
from transformers import AutoModel, AutoTokenizer, BitsAndBytesConfig
import re

# --- –ù–ê–°–¢–†–û–ô–ö–ò ---
MODEL_PATH = './qolda_model_local'  # –ü—É—Ç—å –∫ –≤–∞—à–µ–π –º–æ–¥–µ–ª–∏
MAX_SLICES = 6                      # –ë–∞–ª–∞–Ω—Å —Å–∫–æ—Ä–æ—Å—Ç–∏ –∏ –∫–∞—á–µ—Å—Ç–≤–∞
USE_4BIT = True                     # 4-–±–∏—Ç–Ω–æ–µ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏–µ –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏
# -----------------

# --- –í–°–ü–û–ú–û–ì–ê–¢–ï–õ–¨–ù–´–ï –§–£–ù–ö–¶–ò–ò (–∏–∑ –ø—Ä–æ—à–ª–æ–≥–æ –∫–æ–¥–∞) ---
IMAGENET_MEAN = (0.485, 0.456, 0.406)
IMAGENET_STD = (0.229, 0.224, 0.225)

def build_transform(input_size):
    MEAN, STD = IMAGENET_MEAN, IMAGENET_STD
    transform = T.Compose([
        T.Lambda(lambda img: img.convert('RGB') if img.mode != 'RGB' else img),
        T.Resize((input_size, input_size), interpolation=InterpolationMode.BICUBIC),
        T.ToTensor(),
        T.Normalize(mean=MEAN, std=STD)
    ])
    return transform

def find_closest_aspect_ratio(aspect_ratio, target_ratios, width, height, image_size):
    best_ratio_diff = float('inf')
    best_ratio = (1, 1)
    area = width * height
    for ratio in target_ratios:
        target_aspect_ratio = ratio[0] / ratio[1]
        ratio_diff = abs(aspect_ratio - target_aspect_ratio)
        if ratio_diff < best_ratio_diff:
            best_ratio_diff = ratio_diff
            best_ratio = ratio
        elif ratio_diff == best_ratio_diff:
            if area > 0.5 * image_size * image_size * ratio[0] * ratio[1]:
                best_ratio = ratio
    return best_ratio

def dynamic_preprocess(image, min_num=1, max_num=12, image_size=448, use_thumbnail=False):
    orig_width, orig_height = image.size
    aspect_ratio = orig_width / orig_height
    target_ratios = set(
        (i, j) for n in range(min_num, max_num + 1)
        for i in range(1, n + 1)
        for j in range(1, n + 1)
        if i * j <= max_num and i * j >= min_num)
    target_ratios = sorted(target_ratios, key=lambda x: x[0] * x[1])
    target_aspect_ratio = find_closest_aspect_ratio(
        aspect_ratio, target_ratios, orig_width, orig_height, image_size)
    target_width = image_size * target_aspect_ratio[0]
    target_height = image_size * target_aspect_ratio[1]
    blocks = target_aspect_ratio[0] * target_aspect_ratio[1]
    resized_img = image.resize((target_width, target_height))
    processed_images = []
    for i in range(blocks):
        box = (
            (i % (target_width // image_size)) * image_size,
            (i // (target_width // image_size)) * image_size,
            ((i % (target_width // image_size)) + 1) * image_size,
            ((i // (target_width // image_size)) + 1) * image_size
        )
        split_img = resized_img.crop(box)
        processed_images.append(split_img)
    if use_thumbnail and len(processed_images) != 1:
        thumbnail_img = image.resize((image_size, image_size))
        processed_images.append(thumbnail_img)
    return processed_images

def clean_output(text):
    text = re.sub(r'<think>.*?</think>', '', text, flags=re.DOTALL)
    return text.strip()

# --- –ó–ê–ì–†–£–ó–ö–ê –ú–û–î–ï–õ–ò (–ì–õ–û–ë–ê–õ–¨–ù–û) ---
print("–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –≤ –ø–∞–º—è—Ç—å... –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø–æ–¥–æ–∂–¥–∏—Ç–µ.")
compute_dtype = torch.float16

quantization_config = None
if USE_4BIT:
    quantization_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_compute_dtype=compute_dtype,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_use_double_quant=True,
    )

try:
    model = AutoModel.from_pretrained(
        MODEL_PATH,
        quantization_config=quantization_config,
        torch_dtype=compute_dtype if not USE_4BIT else None,
        low_cpu_mem_usage=True,
        trust_remote_code=True,
        device_map="auto"
    ).eval()
    
    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True, use_fast=False)
    print("–ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞!")
except Exception as e:
    print(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –º–æ–¥–µ–ª–∏: {e}")
    exit(1)

# --- –§–£–ù–ö–¶–ò–Ø –ò–ù–§–ï–†–ï–ù–°–ê ---
def predict(image):
    if image is None:
        return "–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –∑–∞–≥—Ä—É–∑–∏—Ç–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ."
    
    try:
        # image –ø—Ä–∏—Ö–æ–¥–∏—Ç —É–∂–µ –∫–∞–∫ PIL –æ–±—ä–µ–∫—Ç –±–ª–∞–≥–æ–¥–∞—Ä—è type='pil' –≤ Gradio
        input_size = 448
        transform = build_transform(input_size=input_size)
        images = dynamic_preprocess(image, image_size=input_size, use_thumbnail=True, max_num=MAX_SLICES)
        pixel_values = [transform(img) for img in images]
        pixel_values = torch.stack(pixel_values)
        
        # –ü–µ—Ä–µ–Ω–æ—Å–∏–º –Ω–∞ GPU –∏ –ø—Ä–∏–≤–æ–¥–∏–º –∫ –Ω—É–∂–Ω–æ–º—É —Ç–∏–ø—É
        pixel_values = pixel_values.to(compute_dtype).cuda()

        generation_config = dict(max_new_tokens=1024, do_sample=False)
        
        # --- –ë–´–õ–û ---
        # question = '<image>\n–†–∞—Å–ø–æ–∑–Ω–∞–π –≤–µ—Å—å —Ä—É–∫–æ–ø–∏—Å–Ω—ã–π —Ç–µ–∫—Å—Ç –Ω–∞ —ç—Ç–æ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏. –í—ã–≤–µ–¥–∏ —Ç–æ–ª—å–∫–æ —Å–∞–º —Ç–µ–∫—Å—Ç.'

        # --- –°–¢–ê–õ–û (–í—Å—Ç–∞–≤–ª—è–µ–º –Ω–æ–≤—ã–π –ø—Ä–æ–º–ø—Ç) ---
        prompt_text = """–¢—ã ‚Äî —Å—Ç—Ä–æ–≥–∏–π —É—á–∏—Ç–µ–ª—å —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞. –¢–≤–æ—è –∑–∞–¥–∞—á–∞ ‚Äî —Ç–æ—á–Ω–æ –ø–µ—Ä–µ–ø–µ—á–∞—Ç–∞—Ç—å —Ç–µ–∫—Å—Ç —É—á–µ–Ω–∏–∫–∞ —Å –∫–∞—Ä—Ç–∏–Ω–∫–∏ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏.
        
        –í–ê–ñ–ù–û:
        1. –ù–ï –ò–°–ü–†–ê–í–õ–Ø–ô –û–®–ò–ë–ö–ò. –ü–∏—à–∏ –≤ —Ç–æ—á–Ω–æ—Å—Ç–∏ —Ç–æ, —á—Ç–æ –≤–∏–¥–∏—à—å (–¥–∞–∂–µ –µ—Å–ª–∏ –Ω–∞–ø–∏—Å–∞–Ω–æ "–º–∞–ª–∞–∫–æ", –ø–∏—à–∏ "–º–∞–ª–∞–∫–æ").
        2. –°–æ—Ö—Ä–∞–Ω—è–π –ø—É–Ω–∫—Ç—É–∞—Ü–∏—é –∞–≤—Ç–æ—Ä–∞ (–Ω–µ –¥–æ–±–∞–≤–ª—è–π –∑–∞–ø—è—Ç—ã–µ, –µ—Å–ª–∏ –∏—Ö –Ω–µ—Ç).
        3. –í—ã–≤–µ–¥–∏ –¢–û–õ–¨–ö–û —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –±–µ–∑ —Å–≤–æ–∏—Ö –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤."""
        
        question = f'<image>\n{prompt_text}'

        with torch.no_grad():
            response = model.chat(tokenizer, pixel_values, question, generation_config)
        
        return clean_output(response)
    
    except Exception as e:
        return f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ: {str(e)}"

# --- –ò–ù–¢–ï–†–§–ï–ô–° GRADIO ---
with gr.Blocks(title="Handwriting OCR") as demo:
    gr.Markdown("# üìù –†–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Ä—É–∫–æ–ø–∏—Å–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ (Local OCR)")
    
    with gr.Row():
        with gr.Column():
            input_image = gr.Image(type="pil", label="–ó–∞–≥—Ä—É–∑–∏—Ç–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ")
            submit_btn = gr.Button("–†–∞—Å–ø–æ–∑–Ω–∞—Ç—å", variant="primary")
        
        with gr.Column():
            # –£–±—Ä–∞–ª–∏ show_copy_button, –¥–æ–±–∞–≤–∏–ª–∏ interactive=False
            output_text = gr.Textbox(label="–†–µ–∑—É–ª—å—Ç–∞—Ç", lines=15, interactive=False)

    submit_btn.click(
        fn=predict,
        inputs=[input_image],
        outputs=[output_text]
    )

    gr.Markdown("–†–∞–±–æ—Ç–∞–µ—Ç –ª–æ–∫–∞–ª—å–Ω–æ –Ω–∞ Qwen-VL/Qolda —Å 4-bit –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π.")

if __name__ == "__main__":
    # share=True —Å–æ–∑–¥–∞—Å—Ç –ø—É–±–ª–∏—á–Ω—É—é —Å—Å—ã–ª–∫—É, –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
    demo.launch(server_name="0.0.0.0", server_port=7860, share=False)